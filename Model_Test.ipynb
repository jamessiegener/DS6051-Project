{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89f2f091-86d5-4ed7-905d-a7a1071c3953",
   "metadata": {},
   "source": [
    "# GPT Model w/o Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a19eba1-e4d4-4e3a-a398-6e9a28476948",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sfs/gpfs/tardis/home/nuf8ms/Documents/MSDS/LLM/DS6051-Project/test/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import ast\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('Playlist_data_with_lyrics.csv')\n",
    "df = pd.read_csv('playlist_data.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47ee3062897c8289"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import ast\n",
    "\n",
    "# Load and parse the dataset\n",
    "\n",
    "# Convert stringified lists to actual lists\n",
    "for col in ['Playlist_Songs', 'Playlist_Artists']:\n",
    "    df[col] = df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "## Training Targets 1st Iteration, 3 epoch Loss ~ 3, 5 epoch Loss ~ 2.5\n",
    "# Create prompt + output text for GPT-2 training\n",
    "# def format_example(row):\n",
    "#     song_lines = [f\"{s} - {a}\" for s, a in zip(row['Playlist_Songs'], row['Playlist_Artists'])]\n",
    "#     return f\"### Prompt: {row['Playlist_Name']}\\n### Playlist:\\n\" + \"\\n\".join(song_lines)\n",
    "\n",
    "## Training Targets 2nd Iteration, 5 epoch Loss ~ 1.3\n",
    "# def format_example(row):\n",
    "#     lines = [f\"[SONG] {s} [ARTIST] {a}\" for s, a in zip(row[\"Playlist_Songs\"], row[\"Playlist_Artists\"])]\n",
    "#     return f\"### Prompt: {row['Playlist_Name']}\\n### Playlist:\\n\" + \"\\n\".join(lines)\n",
    "\n",
    "## Training targets 3rd Iteration, 5 epoch Loss ~ \n",
    "def format_example(row):\n",
    "    lines = [f\"[SONG] {song} [ARTIST] {artist}\" for song, artist in zip(row[\"Playlist_Songs\"], row[\"Playlist_Artists\"])]\n",
    "    playlist_body = \"\\n\".join(lines)\n",
    "    \n",
    "    # Include Playlist_Description in the prompt for training only\n",
    "    return (\n",
    "        f\"### Prompt: {row['Playlist_Name']}\\n\"\n",
    "        f\"### Description: {row['Playlist_Description']}\\n\"\n",
    "        f\"### Playlist:\\n{playlist_body}\"\n",
    "    )\n",
    "\n",
    "df['text'] = df.apply(format_example, axis=1)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_dict({\"text\": df['text'].tolist()})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4429c16525c03280"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "model.resize_token_embeddings(len(tokenizer))  # In case we add special tokens\n",
    "\n",
    "# Tokenize\n",
    "def tokenize(batch):\n",
    "    encodings = tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    encodings[\"labels\"] = encodings[\"input_ids\"].copy()  # ðŸ”¥ Add this line\n",
    "    return encodings\n",
    "    \n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "127fbb7400f10829"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_playlist_model_w_Descriptions\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    fp16=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1044a9f04162890"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_playlist(prompt, max_length=200):\n",
    "    input_text = f\"### Prompt: {prompt}\\n### Playlist:\\n\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids = input_ids.to('cuda'),\n",
    "        max_length=max_length,\n",
    "        temperature=0.9,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return result.split(\"### Playlist:\\n\")[1].strip()\n",
    "\n",
    "# Try it out!\n",
    "prompt = \"James broke his computer\"\n",
    "print(generate_playlist(prompt))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21facbb9720d2f78"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## Import Saved Model and use with generate_playlist function\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2_playlist_model_w_Descriptions/checkpoint-5725\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2_playlist_model_w_Descriptions/checkpoint-5725\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fec26f3e4f20ff1e"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1254fb26-40fa-45fc-a0a0-d8e5f21a3ba8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sfs/gpfs/tardis/home/nuf8ms/Documents/MSDS/LLM/DS6051-Project'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5de06309-1df8-493e-9878-3eebe34f8c64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['checkpoint-5725', 'checkpoint-5500']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71d2b5-c544-4b0c-b9df-4245b94bc878",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# GPT Model w/ Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b419fc2-fc18-4e17-b071-fa13e2b6f572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (test)",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
