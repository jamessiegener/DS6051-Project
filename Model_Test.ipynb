{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df74734c-8768-49dd-bf4c-7d78b2ad51e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/test/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59a58bdb-f1b0-4c01-bde9-e140369d642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Playlist_data_with_lyrics.csv')\n",
    "artists = pd.read_csv('artists.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00c77408-07b8-4954-9484-fa0ca56deff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Playlist_ID</th>\n",
       "      <th>Playlist_Name</th>\n",
       "      <th>Playlist_Songs</th>\n",
       "      <th>Playlist_Artists</th>\n",
       "      <th>Playlist_Song_IDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6mtYuOxzl58vSGnEDtZ9uB</td>\n",
       "      <td>Pop Hits 2000s ‚Äì 2025</td>\n",
       "      <td>['Into You', 'Glad You Came', 'Dark Horse', 'W...</td>\n",
       "      <td>['Ariana Grande', 'The Wanted', 'Katy Perry', ...</td>\n",
       "      <td>['2meEiZKWkiN28gITzFwQo5', '1OXfWI3FQMdsKKC6lk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34NbomaTu7YuOYnky8nLXL</td>\n",
       "      <td>Pop Hits 2025 (Top 50)</td>\n",
       "      <td>['Die With A Smile', 'APT.', 'Espresso', \"we c...</td>\n",
       "      <td>['Lady Gaga', 'ROS√â', 'Sabrina Carpenter', 'Ar...</td>\n",
       "      <td>['2plbrEY59IikOBgBGLjaoe', '5vNRhkKd0yEAg8suGB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4Jb4PDWREzNnbZcOHPcZPy</td>\n",
       "      <td>COUNTRY HITS 2025 üî• New Country Songs + Top Hits</td>\n",
       "      <td>['I Had Some Help (Feat. Morgan Wallen)', \"Aus...</td>\n",
       "      <td>['Post Malone', 'Dasha', 'mgk', 'Dylan Marlowe...</td>\n",
       "      <td>['5IZXB5IKAD2qlvTPJYDCFB', '2uqYupMHANxnwgeiXT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1Cgey68pUlQGsCPI2wJuxr</td>\n",
       "      <td>Best of 2025 üî• Most Popular Hits 2025 Hits</td>\n",
       "      <td>['APT.', 'Anxiety', 'Die With A Smile', 'Messy...</td>\n",
       "      <td>['ROS√â', 'Doechii', 'Lady Gaga', 'Lola Young',...</td>\n",
       "      <td>['5vNRhkKd0yEAg8suGBpjeY', '1musbempyJAw5gfSKZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2L2HwKRvUgBv1YetudaRI3</td>\n",
       "      <td>Pop 2000-2010 Bangers</td>\n",
       "      <td>['Whatcha Say', 'Airplanes (feat. Hayley Willi...</td>\n",
       "      <td>['Jason Derulo', 'B.o.B', 'Bruno Mars', 'Tinch...</td>\n",
       "      <td>['7xkQdy0cy5ymoWT7nedvLz', '1QnvpPFP4Q3FHbDchq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1821</th>\n",
       "      <td>6qyyWabXbr3InAGVlY6xb5</td>\n",
       "      <td>Hype Russian songs</td>\n",
       "      <td>['–î–£–õ–û', '–ù–æ–≤–∞—è –≤–æ–ª–Ω–∞', 'Cristal &amp; –ú–û–Å–¢', 'FEE...</td>\n",
       "      <td>['MORGENSHTERN', 'DJ SMASH', 'MORGENSHTERN', '...</td>\n",
       "      <td>['2Db7e6H4R5XXnyuTtxUggp', '7rRzijIIKQ8GntJ8IU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>1n7fMfBvijF4JBkFekFVpE</td>\n",
       "      <td>Learn Russian in Your Car</td>\n",
       "      <td>['Introduction', 'Lesson 1 - Basics', 'Lesson ...</td>\n",
       "      <td>['Henry Raymond Jr.', 'Henry Raymond Jr.', 'He...</td>\n",
       "      <td>['4bmotjQ98b8OfUvHvSKHTh', '263DVJyCMhSgLy7UNj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1823</th>\n",
       "      <td>2Y8I4KdIOYCLZoEmrUQj1o</td>\n",
       "      <td>Russian Roulette</td>\n",
       "      <td>['House Of The Rising Sun', 'KICK BACK', 'RUN ...</td>\n",
       "      <td>['The Animals', 'Kenshi Yonezu', 'iamjakehill'...</td>\n",
       "      <td>['7BY005dacJkbO6EPiOh2wb', '3khEEPRyBeOUabbmOP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1824</th>\n",
       "      <td>5y8oqVJhiYEhAm3xkv7oob</td>\n",
       "      <td>russian roulette</td>\n",
       "      <td>['Die With A Smile', 'Die With A Smile', 'Die ...</td>\n",
       "      <td>['Lady Gaga', 'Lady Gaga', 'Lady Gaga', 'KSI',...</td>\n",
       "      <td>['2plbrEY59IikOBgBGLjaoe', '2plbrEY59IikOBgBGL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>0ywfKj3vXpg2kj2Q3NLZL2</td>\n",
       "      <td>Russian viral musicsüá∑üá∫</td>\n",
       "      <td>['–¢–ª–µ–µ—Ç', 'EMPIRE', '–î–∏–∫–∞—è –ª—å–≤–∏—Ü–∞', 'Resurrect...</td>\n",
       "      <td>['Bula', 'Ogryzek', 'ALEX&amp;RUS', 'GANK', 'Konfu...</td>\n",
       "      <td>['0F7pTAMyTJFdvveeQ1GfVL', '7vJ8crYEBNT45NNAdn...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1826 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Playlist_ID  \\\n",
       "0     6mtYuOxzl58vSGnEDtZ9uB   \n",
       "1     34NbomaTu7YuOYnky8nLXL   \n",
       "2     4Jb4PDWREzNnbZcOHPcZPy   \n",
       "3     1Cgey68pUlQGsCPI2wJuxr   \n",
       "4     2L2HwKRvUgBv1YetudaRI3   \n",
       "...                      ...   \n",
       "1821  6qyyWabXbr3InAGVlY6xb5   \n",
       "1822  1n7fMfBvijF4JBkFekFVpE   \n",
       "1823  2Y8I4KdIOYCLZoEmrUQj1o   \n",
       "1824  5y8oqVJhiYEhAm3xkv7oob   \n",
       "1825  0ywfKj3vXpg2kj2Q3NLZL2   \n",
       "\n",
       "                                         Playlist_Name  \\\n",
       "0                                Pop Hits 2000s ‚Äì 2025   \n",
       "1                               Pop Hits 2025 (Top 50)   \n",
       "2     COUNTRY HITS 2025 üî• New Country Songs + Top Hits   \n",
       "3           Best of 2025 üî• Most Popular Hits 2025 Hits   \n",
       "4                                Pop 2000-2010 Bangers   \n",
       "...                                                ...   \n",
       "1821                               Hype Russian songs    \n",
       "1822                         Learn Russian in Your Car   \n",
       "1823                                  Russian Roulette   \n",
       "1824                                  russian roulette   \n",
       "1825                            Russian viral musicsüá∑üá∫   \n",
       "\n",
       "                                         Playlist_Songs  \\\n",
       "0     ['Into You', 'Glad You Came', 'Dark Horse', 'W...   \n",
       "1     ['Die With A Smile', 'APT.', 'Espresso', \"we c...   \n",
       "2     ['I Had Some Help (Feat. Morgan Wallen)', \"Aus...   \n",
       "3     ['APT.', 'Anxiety', 'Die With A Smile', 'Messy...   \n",
       "4     ['Whatcha Say', 'Airplanes (feat. Hayley Willi...   \n",
       "...                                                 ...   \n",
       "1821  ['–î–£–õ–û', '–ù–æ–≤–∞—è –≤–æ–ª–Ω–∞', 'Cristal & –ú–û–Å–¢', 'FEE...   \n",
       "1822  ['Introduction', 'Lesson 1 - Basics', 'Lesson ...   \n",
       "1823  ['House Of The Rising Sun', 'KICK BACK', 'RUN ...   \n",
       "1824  ['Die With A Smile', 'Die With A Smile', 'Die ...   \n",
       "1825  ['–¢–ª–µ–µ—Ç', 'EMPIRE', '–î–∏–∫–∞—è –ª—å–≤–∏—Ü–∞', 'Resurrect...   \n",
       "\n",
       "                                       Playlist_Artists  \\\n",
       "0     ['Ariana Grande', 'The Wanted', 'Katy Perry', ...   \n",
       "1     ['Lady Gaga', 'ROS√â', 'Sabrina Carpenter', 'Ar...   \n",
       "2     ['Post Malone', 'Dasha', 'mgk', 'Dylan Marlowe...   \n",
       "3     ['ROS√â', 'Doechii', 'Lady Gaga', 'Lola Young',...   \n",
       "4     ['Jason Derulo', 'B.o.B', 'Bruno Mars', 'Tinch...   \n",
       "...                                                 ...   \n",
       "1821  ['MORGENSHTERN', 'DJ SMASH', 'MORGENSHTERN', '...   \n",
       "1822  ['Henry Raymond Jr.', 'Henry Raymond Jr.', 'He...   \n",
       "1823  ['The Animals', 'Kenshi Yonezu', 'iamjakehill'...   \n",
       "1824  ['Lady Gaga', 'Lady Gaga', 'Lady Gaga', 'KSI',...   \n",
       "1825  ['Bula', 'Ogryzek', 'ALEX&RUS', 'GANK', 'Konfu...   \n",
       "\n",
       "                                      Playlist_Song_IDs  \n",
       "0     ['2meEiZKWkiN28gITzFwQo5', '1OXfWI3FQMdsKKC6lk...  \n",
       "1     ['2plbrEY59IikOBgBGLjaoe', '5vNRhkKd0yEAg8suGB...  \n",
       "2     ['5IZXB5IKAD2qlvTPJYDCFB', '2uqYupMHANxnwgeiXT...  \n",
       "3     ['5vNRhkKd0yEAg8suGBpjeY', '1musbempyJAw5gfSKZ...  \n",
       "4     ['7xkQdy0cy5ymoWT7nedvLz', '1QnvpPFP4Q3FHbDchq...  \n",
       "...                                                 ...  \n",
       "1821  ['2Db7e6H4R5XXnyuTtxUggp', '7rRzijIIKQ8GntJ8IU...  \n",
       "1822  ['4bmotjQ98b8OfUvHvSKHTh', '263DVJyCMhSgLy7UNj...  \n",
       "1823  ['7BY005dacJkbO6EPiOh2wb', '3khEEPRyBeOUabbmOP...  \n",
       "1824  ['2plbrEY59IikOBgBGLjaoe', '2plbrEY59IikOBgBGL...  \n",
       "1825  ['0F7pTAMyTJFdvveeQ1GfVL', '7vJ8crYEBNT45NNAdn...  \n",
       "\n",
       "[1826 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa5a839c-8956-4094-968b-606271d81f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert stringified lists to Python lists if needed\n",
    "import ast\n",
    "for col in ['Playlist_Songs', 'Playlist_Artists', 'Playlist_Song_IDs']:\n",
    "    df[col] = df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Build target output string: \"Song1 by Artist1, Song2 by Artist2, ...\"\n",
    "def build_output_text(row):\n",
    "    songs = row['Playlist_Songs']\n",
    "    artists = row['Playlist_Artists']\n",
    "    paired = [f\"{s} by {a}\" for s, a in zip(songs, artists)]\n",
    "    return ', '.join(paired)\n",
    "\n",
    "# Build final dataset\n",
    "df['input_text'] = df['Playlist_Name']\n",
    "df['target_text'] = df.apply(build_output_text, axis=1)\n",
    "\n",
    "# Split into train and validation\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['input_text'].tolist(),\n",
    "    df['target_text'].tolist(),\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "train_data = Dataset.from_dict({'input_text': train_texts, 'label': train_labels})\n",
    "val_data = Dataset.from_dict({'input_text': val_texts, 'label': val_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8afe4be6-a9c1-48bc-9c13-b2495b4d3c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1460/1460 [00:01<00:00, 1047.41 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366/366 [00:00<00:00, 1082.65 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Tokenize input and label\n",
    "def tokenize(batch):\n",
    "    input_encodings = tokenizer(batch['input_text'], padding=\"max_length\", truncation=True, max_length=64)\n",
    "    label_encodings = tokenizer(batch['label'], padding=\"max_length\", truncation=True, max_length=256)\n",
    "    input_encodings[\"labels\"] = label_encodings[\"input_ids\"]\n",
    "    return input_encodings\n",
    "\n",
    "train_data = train_data.map(tokenize, batched=True)\n",
    "val_data = val_data.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0273bb21-41ba-45a5-97ee-0340d5bb1769",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments, Trainer\n\u001b[0;32m----> 3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./t5_playlist_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mno\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./logs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     16\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     17\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_data,\n\u001b[1;32m     18\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mval_data\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5_playlist_model\",\n",
    "    evaluation_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fa3b57-7561-4048-8a21-85b8ee0ef517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_playlist(prompt, max_length=128):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    output = model.generate(input_ids, max_length=max_length, temperature=0.8)\n",
    "    decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return decoded\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"Road trip anthems for 2020s pop hits\"\n",
    "print(generate_playlist(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f2f091-86d5-4ed7-905d-a7a1071c3953",
   "metadata": {},
   "source": [
    "# GPT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "270efa3e-de10-4730-afca-f9752a0f8024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import ast\n",
    "\n",
    "# Load and parse the dataset\n",
    "\n",
    "# Convert stringified lists to actual lists\n",
    "for col in ['Playlist_Songs', 'Playlist_Artists']:\n",
    "    df[col] = df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Create prompt + output text for GPT-2 training\n",
    "def format_example(row):\n",
    "    song_lines = [f\"{s} - {a}\" for s, a in zip(row['Playlist_Songs'], row['Playlist_Artists'])]\n",
    "    return f\"### Prompt: {row['Playlist_Name']}\\n### Playlist:\\n\" + \"\\n\".join(song_lines)\n",
    "\n",
    "df['text'] = df.apply(format_example, axis=1)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_dict({\"text\": df['text'].tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a9fcd7f-fe4f-487a-99ff-c9c48600e881",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1826/1826 [00:03<00:00, 540.88 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))  # In case we add special tokens\n",
    "\n",
    "# Tokenize\n",
    "def tokenize(batch):\n",
    "    encodings = tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    encodings[\"labels\"] = encodings[\"input_ids\"].copy()  # üî• Add this line\n",
    "    return encodings\n",
    "    \n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8b1a832-3911-48d6-8d45-3b84f2394e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2739' max='2739' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2739/2739 12:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.733000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.368300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.211100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.343100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.254700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.105400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.167800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.231800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.126600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.014500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3.096300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.981600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.764900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.947400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.023100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.952900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>2.945600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.955300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>2.902600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.690500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2.877500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.823000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>2.793100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.688400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>2.822200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.797200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>2.825100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.750900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>2.834100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.731200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>2.726000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.785700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>2.710900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.633600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>2.829300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.709200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>2.697100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.708300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>2.756200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.730700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>2.580800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.597200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>2.628900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.743400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>2.661500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.631900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>2.759600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.636400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>2.518400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.596800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>2.521600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.542700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>2.593700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.535000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2739, training_loss=2.842794233797762, metrics={'train_runtime': 744.0231, 'train_samples_per_second': 7.363, 'train_steps_per_second': 3.681, 'total_flos': 1431357751296000.0, 'train_loss': 2.842794233797762, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_playlist_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    fp16=True if torch.cuda.is_available() else False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5ad9200-c296-4dfa-aef3-3f60f9e76d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American Gods - Neil Diamond\n",
      "Born to Run - Daryl Hall & John Mellencamp\n",
      "The Greatest Show - Daryl Hall & John Mellencamp\n",
      "Mr. Blue Sky - Daryl Hall & John Mellencamp\n",
      "Cars - Daryl Hall & John Mellencamp\n",
      "The Good, The Bad And The Ugly - Daryl Hall & John Mellencamp\n",
      "All of the Above - Daryl Hall & John Mellencamp\n",
      "Locked in the Cold - Daryl Hall & John Mellencamp\n",
      "So Here's To You - Daryl Hall & John Mellencamp\n",
      "The Last Night - Daryl Hall & John Mellencamp\n",
      "The Power Of Love - Daryl Hall & John Mellencamp\n",
      "You're a Fine Girl - Daryl Hall & John Mellencamp\n",
      "The Night We Met - Daryl Hall & John Mellencamp\n",
      "The Night We Met Tonight - Daryl Hall & John Mellencamp\n",
      "We Are The World - Bruce Spring\n"
     ]
    }
   ],
   "source": [
    "def generate_playlist(prompt, max_length=200):\n",
    "    input_text = f\"### Prompt: {prompt}\\n### Playlist:\\n\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids = input_ids.to('mps'),\n",
    "        max_length=max_length,\n",
    "        temperature=0.9,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return result.split(\"### Playlist:\\n\")[1].strip()\n",
    "\n",
    "# Try it out!\n",
    "prompt = \"90s Grunge\"\n",
    "print(generate_playlist(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9de8892a-453c-4155-9695-80f85cb0499e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1826"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40c068b-245f-4503-800a-bd7adc185e73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
