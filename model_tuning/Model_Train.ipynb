{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89f2f091-86d5-4ed7-905d-a7a1071c3953",
   "metadata": {},
   "source": [
    "# GPT Model w/ Descriptions\n",
    "Combined_Songs_Artists.csv\n",
    "\n",
    "Model trained on Playlist_Names, Playlist_Descriptions, Songs and Artists combined in one string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a19eba1-e4d4-4e3a-a398-6e9a28476948",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Required Libraries\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import ast\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import os\n",
    "import csv\n",
    "import optuna\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770120f8",
   "metadata": {},
   "source": [
    "## Load and Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a58bdb-f1b0-4c01-bde9-e140369d642a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Additional Data/Combined_Songs_Artists.csv') ## Use your dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270efa3e-de10-4730-afca-f9752a0f8024",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "df[\"Playlist_Songs\"] = df[\"Playlist_Songs\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "# Convert the playlist songs to a string format\n",
    "def format_example(row):\n",
    "    # Use the song-artist strings as-is\n",
    "    playlist_body = \"\\n\".join(row[\"Playlist_Songs\"])\n",
    "    return (\n",
    "        f\"### Prompt: {row['Playlist_Name']}\\n\"\n",
    "        f\"### Description: {row['Playlist_Description']}\\n\"\n",
    "        f\"### Playlist:\\n{playlist_body}\"\n",
    "    )\n",
    "\n",
    "# Apply the formatting function to each row\n",
    "df['text'] = df.apply(format_example, axis=1)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_dict({\"text\": df['text'].tolist()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8e1f88",
   "metadata": {},
   "source": [
    "## Load and FineTune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9fcd7f-fe4f-487a-99ff-c9c48600e881",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2289/2289 [00:07<00:00, 310.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "model.resize_token_embeddings(len(tokenizer))  # In case we add special tokens\n",
    "\n",
    "# Definie a function to tokenize the dataset\n",
    "def tokenize(batch):\n",
    "    encodings = tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    encodings[\"labels\"] = encodings[\"input_ids\"].copy()\n",
    "    return encodings\n",
    "    \n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Split the dataset into training and evaluation sets\n",
    "# The dataset is split into 90% training and 10% evaluation\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b1a832-3911-48d6-8d45-3b84f2394e9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./Models/gpt2_Combined_Song_Artists_Eval_Checkpoints\",\n",
    "    overwrite_output_dir=True,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=100,\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    fp16=True if torch.cuda.is_available() else False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='loss',\n",
    "    greater_is_better=False\n",
    "    \n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e2baa7",
   "metadata": {},
   "source": [
    "## Save the FineTuned Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26b846bc-9359-43e9-b049-825183e2befe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/gpt2_Combined_Song_Artists/tokenizer_config.json',\n",
       " './models/gpt2_Combined_Song_Artists/special_tokens_map.json',\n",
       " './models/gpt2_Combined_Song_Artists/vocab.json',\n",
       " './models/gpt2_Combined_Song_Artists/merges.txt',\n",
       " './models/gpt2_Combined_Song_Artists/added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./models/gpt2_Combined_Song_Artists\")\n",
    "tokenizer.save_pretrained(\"./models/gpt2_Combined_Song_Artists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ad9200-c296-4dfa-aef3-3f60f9e76d5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to generate playlists\n",
    "def generate_playlist(prompt, max_length=200):\n",
    "    input_text = f\"### Prompt: {prompt}\\n### Playlist:\\n\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    output = model.generate(\n",
    "        input_ids = input_ids.to('cuda'),\n",
    "        max_length=max_length,\n",
    "        temperature=0.9,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    # Decode the generated output\n",
    "    result = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    # Extract the playlist from the generated text\n",
    "    return result.split(\"### Playlist:\\n\")[1].strip()\n",
    "\n",
    "# Try it out!\n",
    "prompt = \"Hype Pregame\"\n",
    "print(generate_playlist(prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
